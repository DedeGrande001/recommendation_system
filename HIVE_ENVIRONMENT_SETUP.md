# Hive ç¯å¢ƒæ­å»ºæŒ‡å—

## ğŸ“‹ ç¯å¢ƒéœ€æ±‚æ¸…å•

### 1ï¸âƒ£ åŸºç¡€ç¯å¢ƒè¦æ±‚

#### æ“ä½œç³»ç»Ÿ
- **æ¨èç³»ç»Ÿ:**
  - Linux: Ubuntu 20.04/22.04 æˆ– CentOS 7/8
  - macOS: 10.14+ (MojaveåŠä»¥ä¸Š)
  - Windows: Windows 10/11 (éœ€è¦WSL2)

- **å½“å‰ä½ çš„ç³»ç»Ÿ:** Windows (ä»é¡¹ç›®è·¯å¾„åˆ¤æ–­)
  - âœ… å¯è¡Œæ–¹æ¡ˆ1: ä½¿ç”¨ **WSL2 (Windows Subsystem for Linux)**
  - âœ… å¯è¡Œæ–¹æ¡ˆ2: ä½¿ç”¨ **Docker** (æœ€ç®€å•)
  - âœ… å¯è¡Œæ–¹æ¡ˆ3: ä½¿ç”¨è™šæ‹Ÿæœº (VirtualBox/VMware)

#### ç¡¬ä»¶è¦æ±‚
```
æœ€ä½é…ç½®ï¼ˆå¼€å‘/æ¼”ç¤ºï¼‰:
- CPU: 4æ ¸å¿ƒ
- å†…å­˜: 8GB RAM
- ç£ç›˜: 50GB å¯ç”¨ç©ºé—´

æ¨èé…ç½®ï¼ˆæ€§èƒ½æµ‹è¯•ï¼‰:
- CPU: 8æ ¸å¿ƒ
- å†…å­˜: 16GB RAM
- ç£ç›˜: 100GB SSD
```

---

## 2ï¸âƒ£ æ ¸å¿ƒè½¯ä»¶ä¾èµ–

### A. Java Development Kit (JDK)

**ç‰ˆæœ¬è¦æ±‚:** JDK 8 æˆ– JDK 11

**æ£€æŸ¥ç°æœ‰Javaç‰ˆæœ¬:**
```bash
java -version
```

**ä½ çš„ç³»ç»Ÿå·²æœ‰JDK 21** (ä» `movielens_processor.py` çœ‹åˆ°):
```python
os.environ['JAVA_HOME'] = r'C:\Program Files\Java\jdk-21'
```

**é‡è¦æç¤º:**
- âœ… Hive 3.1+ æ”¯æŒ JDK 11
- âš ï¸ JDK 21 å¯èƒ½æœ‰å…¼å®¹æ€§é—®é¢˜
- å»ºè®®: å®‰è£… JDK 11 LTS ç‰ˆæœ¬å¹¶è¡Œä½¿ç”¨

**å®‰è£…JDK 11 (Windows):**
1. ä¸‹è½½: https://adoptium.net/temurin/releases/?version=11
2. é€‰æ‹© Windows x64 MSI å®‰è£…åŒ…
3. å®‰è£…åˆ°: `C:\Program Files\Java\jdk-11`
4. é…ç½®ç¯å¢ƒå˜é‡ (å¯åˆ‡æ¢):
   ```bash
   JAVA_HOME=C:\Program Files\Java\jdk-11
   PATH=%JAVA_HOME%\bin;%PATH%
   ```

---

### B. Apache Hadoop (HDFS + YARN)

**ç‰ˆæœ¬è¦æ±‚:** Hadoop 3.3.x

**æ ¸å¿ƒç»„ä»¶:**
1. **HDFS** (Hadoop Distributed File System) - åˆ†å¸ƒå¼æ–‡ä»¶å­˜å‚¨
2. **YARN** (Yet Another Resource Negotiator) - èµ„æºç®¡ç†å™¨ (å¯é€‰ï¼ŒHiveå¯ä»¥ä¸ä¾èµ–YARN)

**å®‰è£…æ–¹å¼é€‰æ‹©:**

#### æ–¹å¼1: ä½¿ç”¨ Docker (å¼ºçƒˆæ¨è â­â­â­â­â­)
```bash
# æ‹‰å–Hadoopé•œåƒ
docker pull apache/hadoop:3.3.6

# è¿è¡ŒHadoopå®¹å™¨
docker run -d \
  --name hadoop \
  -p 9870:9870 \
  -p 8088:8088 \
  -p 9000:9000 \
  apache/hadoop:3.3.6
```

**ä¼˜ç‚¹:**
- æ— éœ€å¤æ‚é…ç½®
- 5åˆ†é’Ÿå³å¯å¯åŠ¨
- éš”ç¦»ç¯å¢ƒï¼Œä¸å½±å“ç°æœ‰ç³»ç»Ÿ

#### æ–¹å¼2: æœ¬åœ°å®‰è£… (Windows WSL2)
```bash
# åœ¨WSL2 Ubuntuä¸­æ‰§è¡Œ
wget https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz
tar -xzf hadoop-3.3.6.tar.gz
sudo mv hadoop-3.3.6 /opt/hadoop

# é…ç½®ç¯å¢ƒå˜é‡
echo 'export HADOOP_HOME=/opt/hadoop' >> ~/.bashrc
echo 'export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin' >> ~/.bashrc
source ~/.bashrc
```

**æœ€å°é…ç½®æ–‡ä»¶:**

`$HADOOP_HOME/etc/hadoop/core-site.xml`:
```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
```

`$HADOOP_HOME/etc/hadoop/hdfs-site.xml`:
```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///opt/hadoop/data/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///opt/hadoop/data/datanode</value>
    </property>
</configuration>
```

**æ ¼å¼åŒ–HDFSå¹¶å¯åŠ¨:**
```bash
# æ ¼å¼åŒ–NameNode (åªéœ€æ‰§è¡Œä¸€æ¬¡)
hdfs namenode -format

# å¯åŠ¨HDFS
start-dfs.sh

# éªŒè¯HDFSè¿è¡Œ
hdfs dfs -ls /
```

---

### C. Apache Hive

**ç‰ˆæœ¬è¦æ±‚:** Hive 3.1.3

**ä¾èµ–æ£€æŸ¥:**
- âœ… Java 8/11 å·²å®‰è£…
- âœ… Hadoop HDFS å·²è¿è¡Œ
- âš ï¸ éœ€è¦å…³ç³»å‹æ•°æ®åº“ (å­˜å‚¨Metastore)

**å®‰è£…æ­¥éª¤:**

#### 1. ä¸‹è½½Hive
```bash
wget https://dlcdn.apache.org/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz
tar -xzf apache-hive-3.1.3-bin.tar.gz
sudo mv apache-hive-3.1.3-bin /opt/hive
```

#### 2. é…ç½®ç¯å¢ƒå˜é‡
```bash
echo 'export HIVE_HOME=/opt/hive' >> ~/.bashrc
echo 'export PATH=$PATH:$HIVE_HOME/bin' >> ~/.bashrc
source ~/.bashrc
```

#### 3. é…ç½®Hive
åˆ›å»º `$HIVE_HOME/conf/hive-site.xml`:
```xml
<configuration>
    <!-- HDFSè·¯å¾„é…ç½® -->
    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse</value>
    </property>

    <!-- Metastoreæ•°æ®åº“é…ç½® (ä½¿ç”¨Derbyå†…åµŒæ•°æ®åº“ - å¼€å‘ç¯å¢ƒ) -->
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:derby:;databaseName=/opt/hive/metastore_db;create=true</value>
    </property>

    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>org.apache.derby.jdbc.EmbeddedDriver</value>
    </property>

    <!-- MetastoreæœåŠ¡é…ç½® -->
    <property>
        <name>hive.metastore.uris</name>
        <value>thrift://localhost:9083</value>
    </property>
</configuration>
```

#### 4. åˆå§‹åŒ–Metastoreæ•°æ®åº“
```bash
# åˆå§‹åŒ–schema (åªéœ€æ‰§è¡Œä¸€æ¬¡)
schematool -dbType derby -initSchema
```

#### 5. å¯åŠ¨Hive MetastoreæœåŠ¡
```bash
# åå°å¯åŠ¨Metastore
nohup hive --service metastore &

# æ£€æŸ¥æœåŠ¡æ˜¯å¦è¿è¡Œ
netstat -an | grep 9083
```

---

### D. Metastore æ•°æ®åº“ (é‡è¦!)

Hiveéœ€è¦ä¸€ä¸ªå…³ç³»å‹æ•°æ®åº“å­˜å‚¨å…ƒæ•°æ®(è¡¨ç»“æ„ã€åˆ†åŒºä¿¡æ¯ç­‰)ã€‚

#### é€‰é¡¹1: Derby (åµŒå…¥å¼æ•°æ®åº“) - å¼€å‘ç¯å¢ƒ
**ä¼˜ç‚¹:**
- æ— éœ€é¢å¤–å®‰è£…
- Hiveè‡ªå¸¦
- é…ç½®ç®€å•

**ç¼ºç‚¹:**
- âš ï¸ **åªæ”¯æŒå•ç”¨æˆ·** (åŒæ—¶åªèƒ½æœ‰ä¸€ä¸ªHiveè¿æ¥)
- ä¸é€‚åˆç”Ÿäº§ç¯å¢ƒ

**é€‚ç”¨åœºæ™¯:** ä¸ªäººå¼€å‘ã€è¯¾ç¨‹æ¼”ç¤º âœ…

#### é€‰é¡¹2: MySQL (æ¨èç”¨äºå°ç»„é¡¹ç›®) â­â­â­â­
**ä¼˜ç‚¹:**
- æ”¯æŒå¤šç”¨æˆ·å¹¶å‘
- ç¨³å®šå¯é 
- æ˜“äºå¤‡ä»½

**å®‰è£…MySQL:**
```bash
# Ubuntu/WSL2
sudo apt update
sudo apt install mysql-server

# å¯åŠ¨MySQL
sudo service mysql start

# åˆ›å»ºHiveä¸“ç”¨æ•°æ®åº“
mysql -u root -p
CREATE DATABASE hive_metastore;
CREATE USER 'hive'@'localhost' IDENTIFIED BY 'hive_password';
GRANT ALL PRIVILEGES ON hive_metastore.* TO 'hive'@'localhost';
FLUSH PRIVILEGES;
EXIT;
```

**ä¸‹è½½MySQL JDBCé©±åŠ¨:**
```bash
cd /opt/hive/lib
wget https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.33/mysql-connector-java-8.0.33.jar
```

**ä¿®æ”¹ `hive-site.xml`:**
```xml
<property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://localhost:3306/hive_metastore?useSSL=false</value>
</property>

<property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.cj.jdbc.Driver</value>
</property>

<property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>hive</value>
</property>

<property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>hive_password</value>
</property>
```

**åˆå§‹åŒ–MySQL Metastore:**
```bash
schematool -dbType mysql -initSchema
```

---

## 3ï¸âƒ£ Spark é›†æˆ Hive

### é…ç½® Spark è¿æ¥ Hive

#### 1. å¤åˆ¶ Hive é…ç½®åˆ° Spark
```bash
cp $HIVE_HOME/conf/hive-site.xml $SPARK_HOME/conf/
```

#### 2. åœ¨ PySpark ä»£ç ä¸­å¯ç”¨ Hive æ”¯æŒ
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("SparkHiveIntegration") \
    .config("spark.sql.warehouse.dir", "/user/hive/warehouse") \
    .config("hive.metastore.uris", "thrift://localhost:9083") \
    .enableHiveSupport() \
    .getOrCreate()

# æµ‹è¯•è¿æ¥
spark.sql("SHOW DATABASES").show()
```

---

## 4ï¸âƒ£ å¿«é€ŸéªŒè¯ç¯å¢ƒ

### å®Œæ•´éªŒè¯è„šæœ¬

```bash
#!/bin/bash

echo "=== ç¯å¢ƒéªŒè¯å¼€å§‹ ==="

# 1. æ£€æŸ¥Java
echo "1. æ£€æŸ¥Javaç‰ˆæœ¬..."
java -version

# 2. æ£€æŸ¥Hadoop HDFS
echo "2. æ£€æŸ¥HDFS..."
hdfs dfs -ls /
if [ $? -eq 0 ]; then
    echo "âœ… HDFSè¿è¡Œæ­£å¸¸"
else
    echo "âŒ HDFSæœªè¿è¡Œï¼Œè¯·æ‰§è¡Œ: start-dfs.sh"
fi

# 3. æ£€æŸ¥Hive Metastore
echo "3. æ£€æŸ¥Hive Metastore..."
netstat -an | grep 9083
if [ $? -eq 0 ]; then
    echo "âœ… Metastoreè¿è¡Œæ­£å¸¸"
else
    echo "âŒ Metastoreæœªè¿è¡Œï¼Œè¯·æ‰§è¡Œ: hive --service metastore &"
fi

# 4. æµ‹è¯•Hive CLI
echo "4. æµ‹è¯•Hive..."
hive -e "SHOW DATABASES;"
if [ $? -eq 0 ]; then
    echo "âœ… Hiveå¯ç”¨"
else
    echo "âŒ Hiveé…ç½®æœ‰è¯¯"
fi

echo "=== éªŒè¯å®Œæˆ ==="
```

---

## 5ï¸âƒ£ æ¨èæ–¹æ¡ˆï¼šDocker Compose ä¸€é”®éƒ¨ç½² (æœ€ç®€å•!)

### ä¸ºä»€ä¹ˆæ¨èDocker?
- âœ… æ— éœ€å¤æ‚é…ç½®
- âœ… æ‰€æœ‰ç»„å‘˜ç¯å¢ƒä¸€è‡´
- âœ… 5åˆ†é’Ÿå¯åŠ¨å®Œæ•´ç¯å¢ƒ
- âœ… ä¸å½±å“ç°æœ‰ç³»ç»Ÿ

### åˆ›å»º `docker-compose.yml`

```yaml
version: '3'

services:
  # Hadoop NameNode
  namenode:
    image: bde2020/hadoop-namenode:2.0.0-hadoop3.2.1-java8
    container_name: namenode
    ports:
      - "9870:9870"
      - "9000:9000"
    environment:
      - CLUSTER_NAME=hive
    env_file:
      - ./hadoop.env

  # Hadoop DataNode
  datanode:
    image: bde2020/hadoop-datanode:2.0.0-hadoop3.2.1-java8
    container_name: datanode
    depends_on:
      - namenode
    environment:
      - SERVICE_PRECONDITION=namenode:9870
    env_file:
      - ./hadoop.env

  # Hive Metastore æ•°æ®åº“ (PostgreSQL)
  hive-metastore-postgresql:
    image: bde2020/hive-metastore-postgresql:2.3.0
    container_name: hive-metastore-postgresql

  # Hive Metastore æœåŠ¡
  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    env_file:
      - ./hadoop.env
    environment:
      - SERVICE_PRECONDITION=namenode:9870 datanode:9864 hive-metastore-postgresql:5432
    ports:
      - "9083:9083"
    depends_on:
      - namenode
      - datanode
      - hive-metastore-postgresql

  # Hive Server (å¯é€‰ï¼Œæä¾›JDBC/ODBCæ¥å£)
  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server
    env_file:
      - ./hadoop.env
    environment:
      - HIVE_CORE_CONF_javax_jdo_option_ConnectionURL=jdbc:postgresql://hive-metastore/metastore
      - SERVICE_PRECONDITION=hive-metastore:9083
    ports:
      - "10000:10000"
    depends_on:
      - hive-metastore
```

### åˆ›å»º `hadoop.env`

```bash
CORE_CONF_fs_defaultFS=hdfs://namenode:9000
CORE_CONF_hadoop_http_staticuser_user=root
CORE_CONF_hadoop_proxyuser_hue_hosts=*
CORE_CONF_hadoop_proxyuser_hue_groups=*

HDFS_CONF_dfs_webhdfs_enabled=true
HDFS_CONF_dfs_permissions_enabled=false
HDFS_CONF_dfs_replication=1

YARN_CONF_yarn_log___aggregation___enable=true
YARN_CONF_yarn_resourcemanager_recovery_enabled=true
YARN_CONF_yarn_resourcemanager_store_class=org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore
YARN_CONF_yarn_resourcemanager_fs_state___store_uri=/rmstate
YARN_CONF_yarn_nodemanager_remote___app___log___dir=/app-logs
YARN_CONF_yarn_log_server_url=http://historyserver:8188/applicationhistory/logs/
YARN_CONF_yarn_timeline___service_enabled=true
YARN_CONF_yarn_timeline___service_generic___application___history_enabled=true
YARN_CONF_yarn_resourcemanager_system___metrics___publisher_enabled=true
YARN_CONF_yarn_resourcemanager_hostname=resourcemanager
YARN_CONF_yarn_timeline___service_hostname=historyserver
YARN_CONF_yarn_resourcemanager_address=resourcemanager:8032
YARN_CONF_yarn_resourcemanager_scheduler_address=resourcemanager:8030
YARN_CONF_yarn_resourcemanager_resource___tracker_address=resourcemanager:8031
```

### å¯åŠ¨ç¯å¢ƒ

```bash
# å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f hive-metastore

# è¿›å…¥Hiveå®¹å™¨æµ‹è¯•
docker exec -it hive-server bash
hive

# åœæ­¢æ‰€æœ‰æœåŠ¡
docker-compose down
```

### éªŒè¯éƒ¨ç½²æˆåŠŸ

```bash
# 1. è®¿é—®Hadoop Web UI
æµè§ˆå™¨æ‰“å¼€: http://localhost:9870

# 2. æµ‹è¯•Hiveè¿æ¥
docker exec -it hive-server hive -e "SHOW DATABASES;"

# 3. ä»Pythonè¿æ¥ (åœ¨ä½ çš„Windowsä¸»æœºä¸Š)
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("TestHive") \
    .config("hive.metastore.uris", "thrift://localhost:9083") \
    .enableHiveSupport() \
    .getOrCreate()

spark.sql("SHOW DATABASES").show()
```

---

## 6ï¸âƒ£ ç½‘ç»œç«¯å£æ¸…å•

ç¡®ä¿ä»¥ä¸‹ç«¯å£æœªè¢«å ç”¨:

| æœåŠ¡ | ç«¯å£ | ç”¨é€” |
|-----|------|------|
| HDFS NameNode Web UI | 9870 | æŸ¥çœ‹HDFSçŠ¶æ€ |
| HDFS NameNode IPC | 9000 | HDFSæ–‡ä»¶æ“ä½œ |
| Hive Metastore | 9083 | Sparkè¿æ¥Hive |
| Hive Server2 | 10000 | JDBC/ODBCè¿æ¥ |
| YARN ResourceManager | 8088 | YARNä»»åŠ¡ç›‘æ§ |

**æ£€æŸ¥ç«¯å£å ç”¨ (Windows):**
```cmd
netstat -ano | findstr "9083"
```

---

## 7ï¸âƒ£ å¸¸è§é—®é¢˜è§£å†³

### Q1: "Connection refused to Metastore"
**åŸå› :** MetastoreæœåŠ¡æœªå¯åŠ¨

**è§£å†³:**
```bash
# æ£€æŸ¥Metastoreè¿›ç¨‹
ps aux | grep metastore

# é‡å¯Metastore
hive --service metastore &
```

### Q2: "Permission denied" è®¿é—®HDFS
**åŸå› :** HDFSæƒé™é—®é¢˜

**è§£å†³:**
```bash
# å…³é—­HDFSæƒé™æ£€æŸ¥ (ä»…å¼€å‘ç¯å¢ƒ)
hdfs dfs -chmod -R 777 /user/hive/warehouse
```

### Q3: Derby "already booted by another instance"
**åŸå› :** Derbyåªæ”¯æŒå•è¿æ¥

**è§£å†³:** åˆ‡æ¢åˆ°MySQL Metastore (è§ä¸Šé¢é…ç½®)

### Q4: Javaç‰ˆæœ¬ä¸å…¼å®¹
**åŸå› :** JDKç‰ˆæœ¬è¿‡é«˜

**è§£å†³:**
```bash
# åˆ‡æ¢åˆ°JDK 11
update-alternatives --config java  # Linux
# æˆ–ä¿®æ”¹JAVA_HOMEç¯å¢ƒå˜é‡
```

---

## 8ï¸âƒ£ æ¨èå­¦ä¹ è·¯å¾„

### Week 9-10: ç¯å¢ƒæ­å»º
1. **Day 1-2:** å®‰è£…Dockerï¼Œè¿è¡Œ docker-compose
2. **Day 3-4:** æµ‹è¯•Hive CLIï¼Œåˆ›å»ºç¬¬ä¸€ä¸ªè¡¨
3. **Day 5-6:** é…ç½®Sparkè¿æ¥Hive
4. **Day 7:** å®Œæ•´éªŒè¯æ•°æ®æµ: CSV â†’ HDFS â†’ Hive â†’ Spark

### Week 11: æ•°æ®æ¸…æ´—å®è·µ
1. ä¸Šä¼ MovieLensæ•°æ®åˆ°HDFS
2. ç¼–å†™Hive SQLæ¸…æ´—è„šæœ¬
3. æµ‹è¯•æ•°æ®è´¨é‡æ£€æŸ¥æŸ¥è¯¢

---

## 9ï¸âƒ£ ç¯å¢ƒé…ç½®æ€»ç»“

### æœ€ç®€æ–¹æ¡ˆ (æ¨èå°ç»„ä½¿ç”¨) â­
```
Docker Compose (æ‰€æœ‰æœåŠ¡å®¹å™¨åŒ–)
    â†“
5åˆ†é’Ÿå¯åŠ¨
    â†“
æ‰€æœ‰ç»„å‘˜ç¯å¢ƒä¸€è‡´
```

**ä¼˜ç‚¹:**
- æœ€å¿«é€Ÿ(5åˆ†é’Ÿ)
- æœ€å¯é (å®˜æ–¹é•œåƒ)
- æœ€æ˜“åä½œ(é…ç½®æ–‡ä»¶å…±äº«)

### å®Œæ•´æœ¬åœ°å®‰è£…æ–¹æ¡ˆ
```
JDK 11
    â†“
Hadoop HDFS
    â†“
MySQL (Metastore)
    â†“
Hive 3.1.3
    â†“
Spark 3.4+
```

**ä¼˜ç‚¹:**
- æ›´æ·±å…¥ç†è§£æ¶æ„
- æ›´çµæ´»çš„é…ç½®

**ç¼ºç‚¹:**
- é…ç½®å¤æ‚(2-3å¤©)
- ç¯å¢ƒå·®å¼‚å¤§

---

## ğŸ”Ÿ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### ç«‹å³æ‰§è¡Œ (ä»Šå¤©):
1. âœ… å®‰è£… Docker Desktop for Windows
2. âœ… ä¸‹è½½æˆ‘æä¾›çš„ `docker-compose.yml`
3. âœ… è¿è¡Œ `docker-compose up -d`
4. âœ… éªŒè¯ Hive å¯ç”¨: `docker exec -it hive-server hive`

### æœ¬å‘¨å®Œæˆ:
1. æµ‹è¯•ä»Pythonè¿æ¥Hive
2. ä¸Šä¼ æµ‹è¯•æ•°æ®åˆ°HDFS
3. åˆ›å»ºç¬¬ä¸€ä¸ªHiveè¡¨

---

## ğŸ“š å‚è€ƒèµ„æ–™

- Hiveå®˜æ–¹æ–‡æ¡£: https://hive.apache.org/
- Hadoopæ–‡æ¡£: https://hadoop.apache.org/docs/stable/
- Docker Hiveé•œåƒ: https://github.com/big-data-europe/docker-hive
- Spark + Hiveé›†æˆ: https://spark.apache.org/docs/latest/sql-data-sources-hive-tables.html

---

**éœ€è¦å¸®åŠ©?**
å¦‚æœé‡åˆ°ä»»ä½•ç¯å¢ƒé…ç½®é—®é¢˜ï¼Œè¯·å‘Šè¯‰æˆ‘å…·ä½“çš„é”™è¯¯ä¿¡æ¯ï¼Œæˆ‘ä¼šå¸®ä½ è§£å†³ï¼
