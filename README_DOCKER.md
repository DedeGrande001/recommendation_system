# Dockerç¯å¢ƒå¿«é€Ÿå¯åŠ¨æŒ‡å—

## ğŸ“¦ åŒ…å«çš„æœåŠ¡

è¿™ä¸ªDocker Composeé…ç½®åŒ…å«ä»¥ä¸‹æœåŠ¡ï¼š

1. **Hadoop HDFS**
   - NameNode (ç«¯å£ 9870, 9000)
   - DataNode

2. **Apache Hive**
   - Metastore (ç«¯å£ 9083)
   - HiveServer2 (ç«¯å£ 10000, 10002)
   - PostgreSQL Metastoreæ•°æ®åº“

---

## ğŸš€ å¿«é€Ÿå¯åŠ¨æ­¥éª¤

### ç¬¬1æ­¥ï¼šç¡®ä¿Dockerå·²å®‰è£…

```bash
# æ£€æŸ¥Dockeræ˜¯å¦å®‰è£…
docker --version
docker-compose --version
```

å¦‚æœæ²¡æœ‰å®‰è£…ï¼š
- ä¸‹è½½ Docker Desktop: https://www.docker.com/products/docker-desktop/
- å®‰è£…åé‡å¯ç”µè„‘

---

### ç¬¬2æ­¥ï¼šå¯åŠ¨æ‰€æœ‰æœåŠ¡

åœ¨é¡¹ç›®æ ¹ç›®å½• `d:\myproject\project\recommendation_system\` ä¸‹æ‰§è¡Œï¼š

```bash
# å¯åŠ¨æ‰€æœ‰æœåŠ¡ï¼ˆåå°è¿è¡Œï¼‰
docker-compose up -d
```

**é¢„æœŸè¾“å‡ºï¼š**
```
Creating network "recommendation_system_default" with the default driver
Creating volume "recommendation_system_hadoop_namenode" with default driver
Creating volume "recommendation_system_hadoop_datanode" with default driver
Creating volume "recommendation_system_hive_postgresql_data" with default driver
Creating namenode ... done
Creating hive-metastore-postgresql ... done
Creating datanode ... done
Creating hive-metastore ... done
Creating hive-server ... done
```

**ç­‰å¾…æ—¶é—´ï¼š** ç¬¬ä¸€æ¬¡å¯åŠ¨éœ€è¦ä¸‹è½½é•œåƒï¼Œå¤§çº¦3-5åˆ†é’Ÿ

---

### ç¬¬3æ­¥ï¼šæŸ¥çœ‹æœåŠ¡çŠ¶æ€

```bash
# æŸ¥çœ‹æ‰€æœ‰å®¹å™¨è¿è¡ŒçŠ¶æ€
docker-compose ps
```

**é¢„æœŸè¾“å‡ºï¼ˆæ‰€æœ‰æœåŠ¡éƒ½åº”è¯¥æ˜¯ "Up"ï¼‰ï¼š**
```
         Name                       Command               State                    Ports
----------------------------------------------------------------------------------------------------------------
datanode                    /entrypoint.sh /run.sh           Up      9864/tcp
hive-metastore              entrypoint.sh /opt/hive/b ...   Up      0.0.0.0:9083->9083/tcp
hive-metastore-postgresql   /docker-entrypoint.sh postgres   Up      5432/tcp
hive-server                 entrypoint.sh /opt/hive/b ...   Up      0.0.0.0:10000->10000/tcp, 0.0.0.0:10002->10002/tcp
namenode                    /entrypoint.sh /run.sh           Up      0.0.0.0:9000->9000/tcp, 0.0.0.0:9870->9870/tcp
```

---

### ç¬¬4æ­¥ï¼šéªŒè¯ç¯å¢ƒ

#### 4.1 éªŒè¯HDFS

```bash
# è¿›å…¥NameNodeå®¹å™¨
docker exec -it namenode bash

# åœ¨å®¹å™¨å†…æ‰§è¡ŒHDFSå‘½ä»¤
hdfs dfs -ls /

# åˆ›å»ºæµ‹è¯•ç›®å½•
hdfs dfs -mkdir -p /user/hive/warehouse

# æŸ¥çœ‹ç›®å½•
hdfs dfs -ls /user/hive/

# é€€å‡ºå®¹å™¨
exit
```

**âœ… æˆåŠŸæ ‡å¿—ï¼š** èƒ½å¤Ÿæ‰§è¡Œå‘½ä»¤ä¸”æ²¡æœ‰æŠ¥é”™

#### 4.2 éªŒè¯Hive

```bash
# è¿›å…¥Hive Serverå®¹å™¨
docker exec -it hive-server bash

# å¯åŠ¨Hive CLI
hive

# åœ¨Hive CLIä¸­æ‰§è¡Œ
hive> SHOW DATABASES;
hive> CREATE DATABASE IF NOT EXISTS movielens_db;
hive> USE movielens_db;
hive> SHOW TABLES;
hive> exit;

# é€€å‡ºå®¹å™¨
exit
```

**âœ… æˆåŠŸæ ‡å¿—ï¼š** èƒ½çœ‹åˆ° `default` æ•°æ®åº“ï¼Œèƒ½åˆ›å»ºæ–°æ•°æ®åº“

#### 4.3 è®¿é—®Webç•Œé¢

åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ï¼š

1. **HDFS NameNode Web UI**
   - URL: http://localhost:9870
   - å¯ä»¥çœ‹åˆ°HDFSçš„å­˜å‚¨çŠ¶æ€ã€æ•°æ®èŠ‚ç‚¹ä¿¡æ¯

2. **HiveServer2 Web UI**
   - URL: http://localhost:10002
   - å¯ä»¥çœ‹åˆ°Hiveçš„è¿è¡ŒçŠ¶æ€

**âœ… æˆåŠŸæ ‡å¿—ï¼š** èƒ½å¤Ÿæ‰“å¼€é¡µé¢ï¼Œçœ‹åˆ°æœåŠ¡è¿è¡Œæ­£å¸¸

---

## ğŸ“‚ HDFSç›®å½•ç»“æ„

å»ºè®®åˆ›å»ºä»¥ä¸‹ç›®å½•ç»“æ„ï¼š

```bash
# æ‰§è¡Œè„šæœ¬åˆ›å»ºç›®å½•
docker exec -it namenode bash -c "
hdfs dfs -mkdir -p /user/hive/warehouse/raw/movies
hdfs dfs -mkdir -p /user/hive/warehouse/raw/ratings
hdfs dfs -mkdir -p /user/hive/warehouse/cleaned
hdfs dfs -chmod -R 777 /user/hive/warehouse
"
```

ç›®å½•è¯´æ˜ï¼š
- `/user/hive/warehouse/raw/movies/` - å­˜æ”¾åŸå§‹ç”µå½±CSVæ–‡ä»¶
- `/user/hive/warehouse/raw/ratings/` - å­˜æ”¾åŸå§‹è¯„åˆ†CSVæ–‡ä»¶
- `/user/hive/warehouse/cleaned/` - å­˜æ”¾æ¸…æ´—åçš„æ•°æ®

---

## ğŸ§ª æµ‹è¯•ä¸Šä¼ æ•°æ®åˆ°HDFS

```bash
# å‡è®¾ä½ æœ‰ä¸€ä¸ªæµ‹è¯•CSVæ–‡ä»¶ test.csv
# ä»Windowsä¸»æœºä¸Šä¼ åˆ°HDFS

# æ–¹æ³•1: å…ˆå¤åˆ¶åˆ°å®¹å™¨ï¼Œå†ä¸Šä¼ åˆ°HDFS
docker cp test.csv namenode:/tmp/test.csv
docker exec -it namenode hdfs dfs -put /tmp/test.csv /user/hive/warehouse/raw/

# æ–¹æ³•2: ç›´æ¥ä»ç®¡é“ä¸Šä¼ ï¼ˆæ¨èï¼‰
cat test.csv | docker exec -i namenode hdfs dfs -put - /user/hive/warehouse/raw/test.csv
```

éªŒè¯ä¸Šä¼ ï¼š
```bash
docker exec -it namenode hdfs dfs -ls /user/hive/warehouse/raw/
docker exec -it namenode hdfs dfs -cat /user/hive/warehouse/raw/test.csv | head -5
```

---

## ğŸ› ï¸ å¸¸ç”¨ç®¡ç†å‘½ä»¤

### æŸ¥çœ‹æ—¥å¿—
```bash
# æŸ¥çœ‹æ‰€æœ‰æœåŠ¡æ—¥å¿—
docker-compose logs

# æŸ¥çœ‹ç‰¹å®šæœåŠ¡æ—¥å¿—
docker-compose logs hive-server
docker-compose logs namenode

# å®æ—¶è·Ÿè¸ªæ—¥å¿—
docker-compose logs -f hive-metastore
```

### é‡å¯æœåŠ¡
```bash
# é‡å¯æ‰€æœ‰æœåŠ¡
docker-compose restart

# é‡å¯ç‰¹å®šæœåŠ¡
docker-compose restart hive-server
```

### åœæ­¢æœåŠ¡
```bash
# åœæ­¢æ‰€æœ‰æœåŠ¡ï¼ˆä¿ç•™æ•°æ®ï¼‰
docker-compose stop

# åœæ­¢å¹¶åˆ é™¤å®¹å™¨ï¼ˆä¿ç•™æ•°æ®å·ï¼‰
docker-compose down

# åœæ­¢å¹¶åˆ é™¤æ‰€æœ‰ï¼ˆåŒ…æ‹¬æ•°æ®å·ï¼‰âš ï¸ æ…ç”¨
docker-compose down -v
```

### é‡æ–°å¯åŠ¨
```bash
# å¦‚æœä¹‹å‰å·²ç»å¯åŠ¨è¿‡ï¼Œå†æ¬¡å¯åŠ¨
docker-compose start

# æˆ–è€…å®Œå…¨é‡å»º
docker-compose up -d --force-recreate
```

---

## ğŸ”§ Pythonè¿æ¥é…ç½®

### ä»æœ¬åœ°Pythonè¿æ¥Hive

```python
from pyspark.sql import SparkSession

# åˆ›å»ºSparkä¼šè¯ï¼Œè¿æ¥åˆ°Dockerä¸­çš„Hive
spark = SparkSession.builder \
    .appName("MovieLensHiveIntegration") \
    .config("spark.sql.warehouse.dir", "hdfs://localhost:9000/user/hive/warehouse") \
    .config("hive.metastore.uris", "thrift://localhost:9083") \
    .enableHiveSupport() \
    .getOrCreate()

# æµ‹è¯•è¿æ¥
spark.sql("SHOW DATABASES").show()

# ä½¿ç”¨æ•°æ®åº“
spark.sql("USE movielens_db")

# æŸ¥çœ‹è¡¨
spark.sql("SHOW TABLES").show()
```

### ä»Pythonä¸Šä¼ æ–‡ä»¶åˆ°HDFS

```python
import subprocess

def upload_to_hdfs(local_path, hdfs_path):
    """ä¸Šä¼ æœ¬åœ°æ–‡ä»¶åˆ°Dockerä¸­çš„HDFS"""
    cmd = f'docker exec -i namenode hdfs dfs -put - {hdfs_path}'

    with open(local_path, 'rb') as f:
        process = subprocess.Popen(
            cmd.split(),
            stdin=f,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE
        )
        stdout, stderr = process.communicate()

    if process.returncode == 0:
        print(f"âœ“ ä¸Šä¼ æˆåŠŸ: {local_path} -> {hdfs_path}")
        return True
    else:
        print(f"âœ— ä¸Šä¼ å¤±è´¥: {stderr.decode()}")
        return False

# ä½¿ç”¨ç¤ºä¾‹
upload_to_hdfs(
    local_path="data/movies.csv",
    hdfs_path="/user/hive/warehouse/raw/movies/movies.csv"
)
```

---

## â“ æ•…éšœæ’æŸ¥

### é—®é¢˜1: å®¹å™¨å¯åŠ¨å¤±è´¥

```bash
# æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
docker-compose logs namenode

# å¸¸è§åŸå› ï¼šç«¯å£è¢«å ç”¨
# æ£€æŸ¥ç«¯å£å ç”¨æƒ…å†µï¼ˆWindowsï¼‰
netstat -ano | findstr "9870"
netstat -ano | findstr "9000"
netstat -ano | findstr "9083"

# è§£å†³æ–¹æ³•ï¼šä¿®æ”¹docker-compose.ymlä¸­çš„ç«¯å£æ˜ å°„
# ä¾‹å¦‚ï¼šå°†9870æ”¹ä¸º9871
```

### é—®é¢˜2: Hive Metastoreè¿æ¥å¤±è´¥

```bash
# æ£€æŸ¥MetastoreæœåŠ¡æ˜¯å¦è¿è¡Œ
docker-compose ps hive-metastore

# æŸ¥çœ‹Metastoreæ—¥å¿—
docker-compose logs hive-metastore

# é‡å¯Metastore
docker-compose restart hive-metastore
docker-compose restart hive-server
```

### é—®é¢˜3: HDFSæƒé™æ‹’ç»

```bash
# å…³é—­HDFSæƒé™æ£€æŸ¥ï¼ˆä»…å¼€å‘ç¯å¢ƒï¼‰
docker exec -it namenode bash -c "
hdfs dfs -chmod -R 777 /user/hive/warehouse
"
```

### é—®é¢˜4: ç£ç›˜ç©ºé—´ä¸è¶³

```bash
# æŸ¥çœ‹Dockerç£ç›˜ä½¿ç”¨æƒ…å†µ
docker system df

# æ¸…ç†æœªä½¿ç”¨çš„é•œåƒå’Œå®¹å™¨
docker system prune -a
```

---

## ğŸ“Š æ€§èƒ½ç›‘æ§

### æŸ¥çœ‹HDFSå­˜å‚¨çŠ¶æ€
```bash
docker exec -it namenode hdfs dfsadmin -report
```

### æŸ¥çœ‹å®¹å™¨èµ„æºä½¿ç”¨
```bash
docker stats
```

---

## ğŸ“ ä¸‹ä¸€æ­¥

ç¯å¢ƒå¯åŠ¨æˆåŠŸåï¼Œç»§ç»­ä»¥ä¸‹æ­¥éª¤ï¼š

1. âœ… åˆ›å»ºHiveè¡¨ï¼ˆè§ `hive_scripts/create_tables.hql`ï¼‰
2. âœ… ä¸Šä¼ MovieLensæ•°æ®åˆ°HDFS
3. âœ… è¿è¡Œæ•°æ®æ¸…æ´—è„šæœ¬
4. âœ… ä»Sparkè¯»å–Hiveè¡¨
5. âœ… ç”Ÿæˆæ¨èç»“æœ

---

## ğŸ“ éœ€è¦å¸®åŠ©ï¼Ÿ

å¦‚æœé‡åˆ°é—®é¢˜ï¼š
1. æ£€æŸ¥ä¸Šé¢çš„"æ•…éšœæ’æŸ¥"éƒ¨åˆ†
2. æŸ¥çœ‹æ—¥å¿—ï¼š`docker-compose logs -f`
3. ç¡®ä¿Docker Desktopæ­£åœ¨è¿è¡Œ
4. ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´ï¼ˆè‡³å°‘10GBï¼‰

---

**ç¯å¢ƒé…ç½®å®Œæˆï¼æ¥ä¸‹æ¥å¯ä»¥å¼€å§‹å¼€å‘Hiveæ•°æ®æ¸…æ´—è„šæœ¬äº†ã€‚** ğŸ‰
