#!/usr/bin/env python3
"""
Hive æ•°æ®æ¸…æ´—ç®¡é“
è‡ªåŠ¨åŒ–æ•°æ®ä¸Šä¼ ã€æ¸…æ´—å’ŒéªŒè¯æµç¨‹
"""

import os
import sys
import subprocess
from pathlib import Path
import logging
import json
from datetime import datetime

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class HiveDataPipeline:
    """Hive æ•°æ®å¤„ç†ç®¡é“"""

    def __init__(self, base_dir=None):
        """åˆå§‹åŒ–"""
        self.base_dir = Path(base_dir) if base_dir else Path(__file__).parent
        self.hive_scripts_dir = self.base_dir / 'hive_scripts'
        self.data_dir = self.base_dir / 'data'
        self.progress_file = self.base_dir / 'hive_cleaning_progress.json'

        # åˆå§‹åŒ–è¿›åº¦
        self.update_progress(0, "åˆå§‹åŒ–", "å‡†å¤‡å¼€å§‹æ•°æ®æ¸…æ´—...")

    def update_progress(self, percentage, current_step, message, status="running"):
        """æ›´æ–°è¿›åº¦ä¿¡æ¯åˆ°æ–‡ä»¶"""
        progress_data = {
            'percentage': percentage,
            'current_step': current_step,
            'message': message,
            'status': status,  # running, completed, error
            'timestamp': datetime.now().isoformat()
        }

        try:
            with open(self.progress_file, 'w', encoding='utf-8') as f:
                json.dump(progress_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"æ— æ³•æ›´æ–°è¿›åº¦æ–‡ä»¶: {e}")

    def run_hdfs_command(self, command):
        """æ‰§è¡Œ HDFS å‘½ä»¤"""
        full_command = f"docker exec namenode hdfs dfs {command}"
        logger.info(f"æ‰§è¡Œ HDFS å‘½ä»¤: {command}")

        try:
            result = subprocess.run(
                full_command,
                shell=True,
                capture_output=True,
                text=True,
                check=True
            )
            logger.info(f"HDFS å‘½ä»¤æ‰§è¡ŒæˆåŠŸ")
            return result.stdout
        except subprocess.CalledProcessError as e:
            logger.error(f"HDFS å‘½ä»¤æ‰§è¡Œå¤±è´¥: {e.stderr}")
            raise

    def run_hive_script(self, script_path):
        """æ‰§è¡Œ Hive SQL è„šæœ¬"""
        script_file = self.hive_scripts_dir / script_path

        if not script_file.exists():
            raise FileNotFoundError(f"è„šæœ¬æ–‡ä»¶ä¸å­˜åœ¨: {script_file}")

        logger.info(f"æ‰§è¡Œ Hive è„šæœ¬: {script_path}")

        # é¦–å…ˆå¤åˆ¶è„šæœ¬åˆ°å®¹å™¨ä¸­
        container_script_path = f"/tmp/{script_path}"
        copy_cmd = f"docker cp {script_file} hive-server:{container_script_path}"

        try:
            subprocess.run(copy_cmd, shell=True, check=True, capture_output=True)
        except subprocess.CalledProcessError as e:
            logger.error(f"å¤åˆ¶è„šæœ¬åˆ°å®¹å™¨å¤±è´¥: {e.stderr}")
            raise

        # ä½¿ç”¨ beeline -f å‚æ•°ä»æ–‡ä»¶æ‰§è¡Œ SQL
        command = f'docker exec hive-server beeline -u jdbc:hive2://localhost:10000 -f {container_script_path}'

        try:
            result = subprocess.run(
                command,
                shell=True,
                capture_output=True,
                text=True,
                timeout=1800  # 30 åˆ†é’Ÿè¶…æ—¶ (æ•°æ®æ¸…æ´—æ­¥éª¤éœ€è¦æ›´é•¿æ—¶é—´)
            )

            if result.returncode == 0:
                logger.info(f"Hive è„šæœ¬æ‰§è¡ŒæˆåŠŸ: {script_path}")
                return result.stdout
            else:
                logger.error(f"Hive è„šæœ¬æ‰§è¡Œå¤±è´¥: {result.stderr}")
                raise Exception(result.stderr)

        except subprocess.TimeoutExpired:
            logger.error(f"Hive è„šæœ¬æ‰§è¡Œè¶…æ—¶: {script_path}")
            raise
        except Exception as e:
            logger.error(f"æ‰§è¡Œ Hive è„šæœ¬æ—¶å‡ºé”™: {str(e)}")
            raise

    def create_hdfs_directories(self):
        """åˆ›å»º HDFS ç›®å½•ç»“æ„"""
        self.update_progress(5, "æ­¥éª¤ 1/6", "åˆ›å»º HDFS ç›®å½•ç»“æ„...")
        logger.info("=" * 60)
        logger.info("æ­¥éª¤ 1: åˆ›å»º HDFS ç›®å½•")
        logger.info("=" * 60)

        directories = [
            '/user/hive/warehouse',
            '/user/hive/warehouse/raw',
            '/user/hive/warehouse/raw/movies',
            '/user/hive/warehouse/raw/ratings',
            '/user/hive/warehouse/cleaned',
            '/user/hive/warehouse/cleaned/movies',
            '/user/hive/warehouse/cleaned/ratings',
        ]

        for directory in directories:
            try:
                # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
                self.run_hdfs_command(f"-test -d {directory}")
                logger.info(f"  âœ“ ç›®å½•å·²å­˜åœ¨: {directory}")
            except:
                # ç›®å½•ä¸å­˜åœ¨ï¼Œåˆ›å»ºå®ƒ
                self.run_hdfs_command(f"-mkdir -p {directory}")
                logger.info(f"  âœ“ åˆ›å»ºç›®å½•: {directory}")

        # è®¾ç½®æƒé™
        self.run_hdfs_command("-chmod -R 777 /user/hive/warehouse")
        logger.info("  âœ“ è®¾ç½®ç›®å½•æƒé™å®Œæˆ")
        self.update_progress(15, "æ­¥éª¤ 1/6", "HDFS ç›®å½•åˆ›å»ºå®Œæˆ")

    def upload_data_to_hdfs(self):
        """ä¸Šä¼ æ•°æ®æ–‡ä»¶åˆ° HDFS"""
        self.update_progress(20, "æ­¥éª¤ 2/6", "ä¸Šä¼ æ•°æ®æ–‡ä»¶åˆ° HDFS...")
        logger.info("=" * 60)
        logger.info("æ­¥éª¤ 2: ä¸Šä¼ æ•°æ®åˆ° HDFS")
        logger.info("=" * 60)

        # æ•°æ®æ–‡ä»¶æ˜ å°„
        data_files = {
            'movies.csv': '/user/hive/warehouse/raw/movies/',
            'ratings.csv': '/user/hive/warehouse/raw/ratings/',
        }

        for local_file, hdfs_path in data_files.items():
            local_path = self.data_dir / local_file

            if not local_path.exists():
                logger.warning(f"  âš  æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨: {local_path}")
                logger.info(f"     è¯·ç¡®ä¿ {local_file} åœ¨ {self.data_dir} ç›®å½•ä¸­")
                continue

            try:
                # æ£€æŸ¥ HDFS ä¸­æ˜¯å¦å·²æœ‰æ–‡ä»¶
                file_in_hdfs = hdfs_path + local_file
                try:
                    self.run_hdfs_command(f"-test -f {file_in_hdfs}")
                    logger.info(f"  âš  æ–‡ä»¶å·²å­˜åœ¨äº HDFS: {file_in_hdfs}")
                    # åˆ é™¤æ—§æ–‡ä»¶
                    self.run_hdfs_command(f"-rm {file_in_hdfs}")
                    logger.info(f"  âœ“ åˆ é™¤æ—§æ–‡ä»¶")
                except:
                    pass

                # ä¸Šä¼ æ–‡ä»¶ï¼ˆä»å®¹å™¨å†…éƒ¨ä¸Šä¼ ï¼‰
                # é¦–å…ˆå¤åˆ¶æ–‡ä»¶åˆ°å®¹å™¨
                copy_cmd = f"docker cp {local_path} namenode:/tmp/{local_file}"
                subprocess.run(copy_cmd, shell=True, check=True)

                # ç„¶åä»å®¹å™¨ä¸Šä¼ åˆ° HDFS
                self.run_hdfs_command(f"-put /tmp/{local_file} {hdfs_path}")

                logger.info(f"  âœ“ ä¸Šä¼ æˆåŠŸ: {local_file} -> {hdfs_path}")

            except Exception as e:
                logger.error(f"  âœ— ä¸Šä¼ å¤±è´¥: {local_file}, é”™è¯¯: {str(e)}")
                raise

        self.update_progress(35, "æ­¥éª¤ 2/6", "æ•°æ®æ–‡ä»¶ä¸Šä¼ å®Œæˆ")

    def create_hive_tables(self):
        """åˆ›å»º Hive è¡¨"""
        self.update_progress(40, "æ­¥éª¤ 3/6", "åˆ›å»º Hive è¡¨ç»“æ„...")
        logger.info("=" * 60)
        logger.info("æ­¥éª¤ 3: åˆ›å»º Hive è¡¨")
        logger.info("=" * 60)

        try:
            output = self.run_hive_script('01_create_tables.sql')
            logger.info("  âœ“ Hive è¡¨åˆ›å»ºæˆåŠŸ")
            self.update_progress(50, "æ­¥éª¤ 3/6", "Hive è¡¨åˆ›å»ºå®Œæˆ")
            return output
        except Exception as e:
            logger.error(f"  âœ— åˆ›å»º Hive è¡¨å¤±è´¥: {str(e)}")
            raise

    def run_data_quality_check(self):
        """è¿è¡Œæ•°æ®è´¨é‡æ£€æŸ¥"""
        self.update_progress(55, "æ­¥éª¤ 4/6", "æ‰§è¡Œæ•°æ®è´¨é‡æ£€æŸ¥...")
        logger.info("=" * 60)
        logger.info("æ­¥éª¤ 4: æ•°æ®è´¨é‡æ£€æŸ¥")
        logger.info("=" * 60)

        try:
            output = self.run_hive_script('02_data_quality_check.sql')
            logger.info("  âœ“ æ•°æ®è´¨é‡æ£€æŸ¥å®Œæˆ")
            self.update_progress(65, "æ­¥éª¤ 4/6", "æ•°æ®è´¨é‡æ£€æŸ¥å®Œæˆ")
            return output
        except Exception as e:
            logger.error(f"  âœ— æ•°æ®è´¨é‡æ£€æŸ¥å¤±è´¥: {str(e)}")
            raise

    def run_data_cleaning(self):
        """è¿è¡Œæ•°æ®æ¸…æ´—"""
        self.update_progress(70, "æ­¥éª¤ 5/6", "æ­£åœ¨æ¸…æ´—æ•°æ®ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
        logger.info("=" * 60)
        logger.info("æ­¥éª¤ 5: æ•°æ®æ¸…æ´—")
        logger.info("=" * 60)

        try:
            output = self.run_hive_script('03_data_cleaning.sql')
            logger.info("  âœ“ æ•°æ®æ¸…æ´—å®Œæˆ")
            self.update_progress(85, "æ­¥éª¤ 5/6", "æ•°æ®æ¸…æ´—å®Œæˆ")
            return output
        except Exception as e:
            logger.error(f"  âœ— æ•°æ®æ¸…æ´—å¤±è´¥: {str(e)}")
            raise

    def verify_results(self):
        """éªŒè¯æœ€ç»ˆç»“æœ"""
        self.update_progress(90, "æ­¥éª¤ 6/6", "éªŒè¯æ¸…æ´—ç»“æœ...")
        logger.info("=" * 60)
        logger.info("æ­¥éª¤ 6: éªŒè¯ç»“æœ")
        logger.info("=" * 60)

        verify_sql = """
        USE movielens_db;
        SELECT 'Tables in database:' as info;
        SHOW TABLES;

        SELECT '' as separator;
        SELECT 'Cleaned Movies Count:' as info;
        SELECT COUNT(*) as count FROM cleaned_movies;

        SELECT '' as separator;
        SELECT 'Cleaned Ratings Count:' as info;
        SELECT COUNT(*) as count FROM cleaned_ratings;

        SELECT '' as separator;
        SELECT 'Sample Cleaned Movies (first 5):' as info;
        SELECT * FROM cleaned_movies LIMIT 5;
        """

        try:
            command = f'docker exec hive-server beeline -u jdbc:hive2://localhost:10000 -e "{verify_sql}"'
            result = subprocess.run(
                command,
                shell=True,
                capture_output=True,
                text=True,
                timeout=60
            )
            logger.info("  âœ“ éªŒè¯å®Œæˆ")
            print("\n" + result.stdout)
            self.update_progress(95, "æ­¥éª¤ 6/6", "éªŒè¯å®Œæˆ")
            return result.stdout
        except Exception as e:
            logger.error(f"  âœ— éªŒè¯å¤±è´¥: {str(e)}")
            raise

    def run_full_pipeline(self):
        """è¿è¡Œå®Œæ•´çš„æ•°æ®ç®¡é“"""
        logger.info("ğŸš€ å¯åŠ¨ Hive æ•°æ®æ¸…æ´—ç®¡é“")
        logger.info("=" * 60)

        try:
            # æ­¥éª¤ 1: åˆ›å»º HDFS ç›®å½•
            self.create_hdfs_directories()

            # æ­¥éª¤ 2: ä¸Šä¼ æ•°æ®
            self.upload_data_to_hdfs()

            # æ­¥éª¤ 3: åˆ›å»ºè¡¨
            self.create_hive_tables()

            # æ­¥éª¤ 4: è´¨é‡æ£€æŸ¥
            self.run_data_quality_check()

            # æ­¥éª¤ 5: æ•°æ®æ¸…æ´—
            self.run_data_cleaning()

            # æ­¥éª¤ 6: éªŒè¯ç»“æœ
            self.verify_results()

            logger.info("=" * 60)
            logger.info("âœ… æ•°æ®ç®¡é“æ‰§è¡Œå®Œæˆï¼")
            logger.info("=" * 60)

            # æ ‡è®°ä¸ºå®Œæˆ
            self.update_progress(100, "å®Œæˆ", "æ‰€æœ‰æ•°æ®æ¸…æ´—æ­¥éª¤å·²å®Œæˆï¼", status="completed")

        except Exception as e:
            logger.error("=" * 60)
            logger.error(f"âŒ æ•°æ®ç®¡é“æ‰§è¡Œå¤±è´¥: {str(e)}")
            logger.error("=" * 60)
            # æ ‡è®°ä¸ºé”™è¯¯
            self.update_progress(0, "é”™è¯¯", f"æ¸…æ´—å¤±è´¥: {str(e)}", status="error")
            sys.exit(1)


def main():
    """ä¸»å‡½æ•°"""
    pipeline = HiveDataPipeline()

    # è¿è¡Œå®Œæ•´ç®¡é“
    pipeline.run_full_pipeline()


if __name__ == '__main__':
    main()
